{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"\")\n",
    "\n",
    "from src.train import eval_model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../config/prod.env\")\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"../config/config.yaml\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from azure.ai.ml import MLClient, command, Input, dsl\n",
    "from azure.ai.ml.entities import JobService\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# enter details of your AML workspace\n",
    "subscription_id = os.getenv(\"subscription_id\")\n",
    "resource_group = os.getenv(\"resource_group\")\n",
    "workspace = os.getenv(\"workspace\")\n",
    "\n",
    "credential = InteractiveBrowserCredential()  # DefaultAzureCredential()\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config/config.yaml\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/training_data_sample.csv\")\n",
    "df.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the command\n",
    "train_job = command(\n",
    "    code=\"../src\",  # local path where the code is stored\n",
    "    command=\"python train.py --input-csv ${{inputs.input_csv}} --random-state ${{inputs.random_state}} --predictor-cols ${{inputs.predictor_cols}} --target-col ${{inputs.target_col}}\",\n",
    "    inputs={\n",
    "        \"input_csv\": Input(\n",
    "            type=\"uri_file\",\n",
    "            path=\"../data/training_data_sample.csv\",  # \"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\n",
    "        ),\n",
    "        \"random_state\": 8,\n",
    "        \"predictor_cols\": \"A,B,C\",\n",
    "        \"target_col\": cfg[\"model\"][\"target_col\"],\n",
    "    },\n",
    "    environment=ml_client.environments.get(\n",
    "        name=cfg[\"environment\"][\"conda\"][\"name\"],\n",
    "        version=cfg[\"environment\"][\"conda\"][\"version\"],\n",
    "    ),\n",
    "    display_name=\"model-training\",\n",
    "    compute=cfg[\"model\"][\"compute\"][\"name\"],\n",
    "    # experiment_name\n",
    "    # description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this doesn't work for some reason\n",
    "# Now we register the component to the workspace\n",
    "train_job_component = ml_client.components.create_or_update(train_job.component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch job\n",
    "returned_job = ml_client.create_or_update(train_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import spark, Input, Output\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"../config/config.yaml\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = \"\"\n",
    "path_on_datastore = \"\"\n",
    "input_uri = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path_on_datastore}\"\n",
    "output_uri_folder = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/spark/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_job = spark(\n",
    "    display_name=\"Test job from serverless Spark with VNet using Data Store\",\n",
    "    code=\"../src/components/spark_step\",\n",
    "    entry={\"file\": \"spark_step.py\"},\n",
    "    driver_cores=1,\n",
    "    driver_memory=\"2g\",\n",
    "    executor_cores=1,\n",
    "    executor_memory=\"1g\",\n",
    "    executor_instances=1,\n",
    "    resources={\n",
    "        \"instance_type\": \"Standard_E4S_V3\",\n",
    "        \"runtime_version\": \"3.3.0\",\n",
    "    },\n",
    "    inputs={\n",
    "        \"input_uri\": Input(\n",
    "            type=\"uri_file\",\n",
    "            path=input_uri,\n",
    "            mode=\"direct\",\n",
    "        ),\n",
    "    },\n",
    "    outputs={\n",
    "        \"output\": Output(\n",
    "            type=\"uri_folder\",\n",
    "            path=output_uri_folder,\n",
    "            mode=\"direct\",\n",
    "        ),\n",
    "    },\n",
    "    args=\"--input_uri ${{inputs.input_uri}} --output ${{outputs.output}}\",\n",
    "    # environment=,\n",
    ")\n",
    "\n",
    "returned_spark_job = ml_client.jobs.create_or_update(spark_job)\n",
    "print(returned_spark_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "components_src_dir = \"../src/components/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load components\n",
    "spark_data_prep = load_component(\n",
    "    source=components_src_dir + \"spark_step/spark_step.yaml\"\n",
    ")\n",
    "data_prep_component = load_component(\n",
    "    source=os.path.join(components_src_dir, \"data_prep/data_prep.yaml\")\n",
    ")\n",
    "train_component = load_component(\n",
    "    source=os.path.join(components_src_dir, \"train/train.yaml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we register the component to the workspace\n",
    "spark_data_prep_component = ml_client.create_or_update(spark_data_prep)\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
    "train_component = ml_client.create_or_update(train_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"\",\n",
    ")\n",
    "def model_training_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_predictor_cols,\n",
    "    pipeline_job_target_col,\n",
    "    pipeline_job_random_state,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        input_csv=pipeline_job_data_input,\n",
    "    )\n",
    "\n",
    "    # using train_func like a python call with its own inputs\n",
    "    train_job = train_component(\n",
    "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
    "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
    "        predictor_cols=pipeline_job_predictor_cols,  # note: using a pipeline input as parameter\n",
    "        target_col=pipeline_job_target_col,\n",
    "        random_state=pipeline_job_random_state,\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_model\": train_job.outputs.model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ml_client.data.get(\n",
    "    name=cfg[\"data\"][\"training\"][\"name\"], version=cfg[\"data\"][\"training\"][\"version\"]\n",
    ")\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline_job = model_training_pipeline(\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=training_data.path),\n",
    "    pipeline_job_random_state=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(pipeline_job, experiment_name=\"\")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-testing-OGfMdhkA-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
